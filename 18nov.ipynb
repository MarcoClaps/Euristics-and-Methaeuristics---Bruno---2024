{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoClaps/Euristics-and-Methaeuristics---Bruno---2024/blob/main/18nov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ1wQDjfNTx1",
        "outputId": "903eba75-1a18-4fb6-8e27-f619304d6087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision -U\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_pLPGgTO9tg",
        "outputId": "4b9e8d05-b032-401f-f4e1-c9b8535f1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "\n",
        "def loadDataSet():\n",
        "    dataMat = []; labelMat = []\n",
        "    fr = open('testSet.txt')\n",
        "    for line in fr.readlines():\n",
        "        lineArr = line.strip().split()\n",
        "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
        "        labelMat.append(int(lineArr[2]))\n",
        "    return dataMat,labelMat\n",
        "\n",
        "def sigmoid(inX):\n",
        "    return 1.0/(1+exp(-inX))\n",
        "\n",
        "def gradAscent(dataMatIn, classLabels):\n",
        "    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix\n",
        "    labelMat = mat(classLabels).transpose() #convert to NumPy matrix\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.001\n",
        "    maxCycles = 500\n",
        "    weights = ones((n,1))\n",
        "    for k in range(maxCycles):              #heavy on matrix operations\n",
        "        h = sigmoid(dataMatrix*weights)     #matrix mult\n",
        "        error = (labelMat - h)              #vector subtraction\n",
        "        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult\n",
        "    return weights\n",
        "\n",
        "def plotBestFit(weights):\n",
        "    import matplotlib.pyplot as plt\n",
        "    dataMat,labelMat=loadDataSet()\n",
        "    dataArr = array(dataMat)\n",
        "    n = shape(dataArr)[0]\n",
        "    xcord1 = []; ycord1 = []\n",
        "    xcord2 = []; ycord2 = []\n",
        "    for i in range(n):\n",
        "        if int(labelMat[i])== 1:\n",
        "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
        "        else:\n",
        "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
        "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
        "    x = arange(-3.0, 3.0, 0.1)\n",
        "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
        "    ax.plot(x, y)\n",
        "    plt.xlabel('X1'); plt.ylabel('X2');\n",
        "    plt.show()\n",
        "\n",
        "def stocGradAscent0(dataMatrix, classLabels):\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.01\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for i in range(m):\n",
        "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
        "        error = classLabels[i] - h\n",
        "        weights = weights + alpha * error * dataMatrix[i]\n",
        "    return weights\n",
        "\n",
        "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
        "    m,n = shape(dataMatrix)\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for j in range(numIter):\n",
        "        dataIndex = range(m)\n",
        "        for i in range(m):\n",
        "            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not\n",
        "            randIndex = int(random.uniform(0, len(dataIndex))) #go to 0 because of the constant\n",
        "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
        "            error = classLabels[randIndex] - h\n",
        "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
        "    return weights\n",
        "\n",
        "def classifyVector(inX, weights):\n",
        "    prob = sigmoid(sum(inX*weights))\n",
        "    if prob > 0.5: return 1.0\n",
        "    else: return 0.0\n",
        "\n",
        "def colicTest():\n",
        "    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n",
        "    trainingSet = []; trainingLabels = []\n",
        "    for line in frTrain.readlines():\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        trainingSet.append(lineArr)\n",
        "        trainingLabels.append(float(currLine[21]))\n",
        "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\n",
        "    errorCount = 0; numTestVec = 0.0\n",
        "    for line in frTest.readlines():\n",
        "        numTestVec += 1.0\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\n",
        "            errorCount += 1\n",
        "    errorRate = (float(errorCount)/numTestVec)\n",
        "    print(\"the error rate of this test is: %f\" % errorRate)\n",
        "    return errorRate\n",
        "\n",
        "def multiTest():\n",
        "    numTests = 10; errorSum=0.0\n",
        "    for k in range(numTests):\n",
        "        errorSum += colicTest()\n",
        "    print(\"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests)))\n",
        "\n",
        "multiTest()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "gVT8Mg9frTJy",
        "outputId": "93ae8b15-b6ee-4b19-dbc9-6c060b932144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'horseColicTraining.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6db5079b0526>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mmultiTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-6db5079b0526>\u001b[0m in \u001b[0;36mmultiTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnumTests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0merrorSum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-6db5079b0526>\u001b[0m in \u001b[0;36mcolicTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mfrTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTraining.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mfrTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTest.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrainingLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'horseColicTraining.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "\n",
        "def loadDataSet():\n",
        "    dataMat = []; labelMat = []\n",
        "    fr = open('testSet.txt')\n",
        "    for line in fr.readlines():\n",
        "        lineArr = line.strip().split()\n",
        "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
        "        labelMat.append(int(lineArr[2]))\n",
        "    return dataMat,labelMat\n",
        "\n",
        "def sigmoid(inX):\n",
        "    return 1.0/(1+exp(-inX))\n",
        "\n",
        "def gradAscent(dataMatIn, classLabels):\n",
        "    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix\n",
        "    labelMat = mat(classLabels).transpose() #convert to NumPy matrix\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.001\n",
        "    maxCycles = 500\n",
        "    weights = ones((n,1))\n",
        "    for k in range(maxCycles):              #heavy on matrix operations\n",
        "        h = sigmoid(dataMatrix*weights)     #matrix mult\n",
        "        error = (labelMat - h)              #vector subtraction\n",
        "        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult\n",
        "    return weights\n",
        "\n",
        "def plotBestFit(weights):\n",
        "    import matplotlib.pyplot as plt\n",
        "    dataMat,labelMat=loadDataSet()\n",
        "    dataArr = array(dataMat)\n",
        "    n = shape(dataArr)[0]\n",
        "    xcord1 = []; ycord1 = []\n",
        "    xcord2 = []; ycord2 = []\n",
        "    for i in range(n):\n",
        "        if int(labelMat[i])== 1:\n",
        "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
        "        else:\n",
        "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
        "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
        "    x = arange(-3.0, 3.0, 0.1)\n",
        "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
        "    ax.plot(x, y)\n",
        "    plt.xlabel('X1'); plt.ylabel('X2');\n",
        "    plt.show()\n",
        "\n",
        "def stocGradAscent0(dataMatrix, classLabels):\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.01\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for i in range(m):\n",
        "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
        "        error = classLabels[i] - h\n",
        "        weights = weights + alpha * error * dataMatrix[i]\n",
        "    return weights\n",
        "\n",
        "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
        "    m,n = shape(dataMatrix)\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for j in range(numIter):\n",
        "        dataIndex = range(m)\n",
        "        for i in range(m):\n",
        "            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not\n",
        "            randIndex = int(random.uniform(0, len(dataIndex))) #go to 0 because of the constant\n",
        "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
        "            error = classLabels[randIndex] - h\n",
        "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
        "    return weights\n",
        "\n",
        "def classifyVector(inX, weights):\n",
        "    prob = sigmoid(sum(inX*weights))\n",
        "    if prob > 0.5: return 1.0\n",
        "    else: return 0.0\n",
        "\n",
        "def colicTest():\n",
        "    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n",
        "    trainingSet = []; trainingLabels = []\n",
        "    for line in frTrain.readlines():\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        trainingSet.append(lineArr)\n",
        "        trainingLabels.append(float(currLine[21]))\n",
        "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\n",
        "    errorCount = 0; numTestVec = 0.0\n",
        "    for line in frTest.readlines():\n",
        "        numTestVec += 1.0\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\n",
        "            errorCount += 1\n",
        "    errorRate = (float(errorCount)/numTestVec)\n",
        "    print(\"the error rate of this test is: %f\" % errorRate)\n",
        "    return errorRate\n",
        "\n",
        "def multiTest():\n",
        "    numTests = 10; errorSum=0.0\n",
        "    for k in range(numTests):\n",
        "        errorSum += colicTest()\n",
        "    print(\"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests)))\n",
        "\n",
        "multiTest()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "n7knokn7rm1V",
        "outputId": "6ff62be4-8908-4685-9b0d-e9f0aefbd1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'horseColicTraining.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6db5079b0526>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mmultiTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-6db5079b0526>\u001b[0m in \u001b[0;36mmultiTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnumTests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0merrorSum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6db5079b0526>\u001b[0m in \u001b[0;36mcolicTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mfrTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTraining.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mfrTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTest.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrainingLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'horseColicTraining.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "\n",
        "def loadDataSet():\n",
        "    dataMat = []; labelMat = []\n",
        "    fr = open('testSet.txt')\n",
        "    for line in fr.readlines():\n",
        "        lineArr = line.strip().split()\n",
        "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
        "        labelMat.append(int(lineArr[2]))\n",
        "    return dataMat,labelMat\n",
        "\n",
        "def sigmoid(inX):\n",
        "    return 1.0/(1+exp(-inX))\n",
        "\n",
        "def gradAscent(dataMatIn, classLabels):\n",
        "    dataMatrix = mat(dataMatIn)             #convert to NumPy matrix\n",
        "    labelMat = mat(classLabels).transpose() #convert to NumPy matrix\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.001\n",
        "    maxCycles = 500\n",
        "    weights = ones((n,1))\n",
        "    for k in range(maxCycles):              #heavy on matrix operations\n",
        "        h = sigmoid(dataMatrix*weights)     #matrix mult\n",
        "        error = (labelMat - h)              #vector subtraction\n",
        "        weights = weights + alpha * dataMatrix.transpose()* error #matrix mult\n",
        "    return weights\n",
        "\n",
        "def plotBestFit(weights):\n",
        "    import matplotlib.pyplot as plt\n",
        "    dataMat,labelMat=loadDataSet()\n",
        "    dataArr = array(dataMat)\n",
        "    n = shape(dataArr)[0]\n",
        "    xcord1 = []; ycord1 = []\n",
        "    xcord2 = []; ycord2 = []\n",
        "    for i in range(n):\n",
        "        if int(labelMat[i])== 1:\n",
        "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
        "        else:\n",
        "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
        "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
        "    x = arange(-3.0, 3.0, 0.1)\n",
        "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
        "    ax.plot(x, y)\n",
        "    plt.xlabel('X1'); plt.ylabel('X2');\n",
        "    plt.show()\n",
        "\n",
        "def stocGradAscent0(dataMatrix, classLabels):\n",
        "    m,n = shape(dataMatrix)\n",
        "    alpha = 0.01\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for i in range(m):\n",
        "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
        "        error = classLabels[i] - h\n",
        "        weights = weights + alpha * error * dataMatrix[i]\n",
        "    return weights\n",
        "\n",
        "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
        "    m,n = shape(dataMatrix)\n",
        "    weights = ones(n)   #initialize to all ones\n",
        "    for j in range(numIter):\n",
        "        dataIndex = range(m)\n",
        "        for i in range(m):\n",
        "            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not\n",
        "            randIndex = int(random.uniform(0, len(dataIndex))) #go to 0 because of the constant\n",
        "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
        "            error = classLabels[randIndex] - h\n",
        "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
        "    return weights\n",
        "\n",
        "def classifyVector(inX, weights):\n",
        "    prob = sigmoid(sum(inX*weights))\n",
        "    if prob > 0.5: return 1.0\n",
        "    else: return 0.0\n",
        "\n",
        "def colicTest():\n",
        "    frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')\n",
        "    trainingSet = []; trainingLabels = []\n",
        "    for line in frTrain.readlines():\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        trainingSet.append(lineArr)\n",
        "        trainingLabels.append(float(currLine[21]))\n",
        "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)\n",
        "    errorCount = 0; numTestVec = 0.0\n",
        "    for line in frTest.readlines():\n",
        "        numTestVec += 1.0\n",
        "        currLine = line.strip().split('\\t')\n",
        "        lineArr =[]\n",
        "        for i in range(21):\n",
        "            lineArr.append(float(currLine[i]))\n",
        "        if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):\n",
        "            errorCount += 1\n",
        "    errorRate = (float(errorCount)/numTestVec)\n",
        "    print(\"the error rate of this test is: %f\" % errorRate)\n",
        "    return errorRate\n",
        "\n",
        "def multiTest():\n",
        "    numTests = 10; errorSum=0.0\n",
        "    for k in range(numTests):\n",
        "        errorSum += colicTest()\n",
        "    print(\"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests)))\n",
        "\n",
        "multiTest()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "AE2uaelTu72o",
        "outputId": "07e41a65-60cc-4f6f-f44e-df8a7cf1e8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'horseColicTraining.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6db5079b0526>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mmultiTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-6db5079b0526>\u001b[0m in \u001b[0;36mmultiTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnumTests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0merrorSum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after %d iterations the average error rate is: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorSum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-6db5079b0526>\u001b[0m in \u001b[0;36mcolicTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolicTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mfrTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTraining.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mfrTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'horseColicTest.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrainingLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'horseColicTraining.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {'batch_size': args.batch_size}\n",
        "    test_kwargs = {'batch_size': args.test_batch_size}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "seXkQCs2xo8k",
        "outputId": "b2c57573-bc69-4b94-aa5a-fb9f9e26a6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--batch-size N] [--test-batch-size N] [--epochs N] [--lr LR]\n",
            "                                [--gamma M] [--no-cuda] [--no-mps] [--dry-run] [--seed S]\n",
            "                                [--log-interval N] [--save-model]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-66ff66aa-021f-4065-a081-c5ed5bc75c51.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Cart Pole\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
        "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
        "                    help='discount factor (default: 0.99)')\n",
        "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
        "                    help='random seed (default: 543)')\n",
        "parser.add_argument('--render', action='store_true',\n",
        "                    help='render the environment')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='interval between training status logs (default: 10)')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.reset(seed=args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    \"\"\"\n",
        "    implements both actor and critic in one model\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(4, 128)\n",
        "\n",
        "        # actor's layer\n",
        "        self.action_head = nn.Linear(128, 2)\n",
        "\n",
        "        # critic's layer\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "        # action & reward buffer\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward of both actor and critic\n",
        "        \"\"\"\n",
        "        x = F.relu(self.affine1(x))\n",
        "\n",
        "        # actor: choses action to take from state s_t\n",
        "        # by returning probability of each action\n",
        "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
        "\n",
        "        # critic: evaluates being in the state s_t\n",
        "        state_values = self.value_head(x)\n",
        "\n",
        "        # return values for both actor and critic as a tuple of 2 values:\n",
        "        # 1. a list with the probability of each action over the action space\n",
        "        # 2. the value from state s_t\n",
        "        return action_prob, state_values\n",
        "\n",
        "\n",
        "model = Policy()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs, state_value = model(state)\n",
        "\n",
        "    # create a categorical distribution over the list of probabilities of actions\n",
        "    m = Categorical(probs)\n",
        "\n",
        "    # and sample an action using the distribution\n",
        "    action = m.sample()\n",
        "\n",
        "    # save to action buffer\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "\n",
        "    # the action to take (left or right)\n",
        "    return action.item()\n",
        "\n",
        "\n",
        "def finish_episode():\n",
        "    \"\"\"\n",
        "    Training code. Calculates actor and critic loss and performs backprop.\n",
        "    \"\"\"\n",
        "    R = 0\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = [] # list to save actor (policy) loss\n",
        "    value_losses = [] # list to save critic (value) loss\n",
        "    returns = [] # list to save the true values\n",
        "\n",
        "    # calculate the true value using rewards returned from the environment\n",
        "    for r in model.rewards[::-1]:\n",
        "        # calculate the discounted value\n",
        "        R = r + args.gamma * R\n",
        "        returns.insert(0, R)\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    for (log_prob, value), R in zip(saved_actions, returns):\n",
        "        advantage = R - value.item()\n",
        "\n",
        "        # calculate actor (policy) loss\n",
        "        policy_losses.append(-log_prob * advantage)\n",
        "\n",
        "        # calculate critic (value) loss using L1 smooth loss\n",
        "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
        "\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # sum up all the values of policy_losses and value_losses\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "\n",
        "    # perform backprop\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # reset rewards and action buffer\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n",
        "\n",
        "\n",
        "def main():\n",
        "    running_reward = 10\n",
        "\n",
        "    # run infinitely many episodes\n",
        "    for i_episode in count(1):\n",
        "\n",
        "        # reset environment and episode reward\n",
        "        state, _ = env.reset()\n",
        "        ep_reward = 0\n",
        "\n",
        "        # for each episode, only run 9999 steps so that we don't\n",
        "        # infinite loop while learning\n",
        "        for t in range(1, 10000):\n",
        "\n",
        "            # select action from policy\n",
        "            action = select_action(state)\n",
        "\n",
        "            # take the action\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            if args.render:\n",
        "                env.render()\n",
        "\n",
        "            model.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # update cumulative reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # perform backprop\n",
        "        finish_episode()\n",
        "\n",
        "        # log results\n",
        "        if i_episode % args.log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "\n",
        "        # check if we have \"solved\" the cart pole problem\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Y1dsoW2dz5wN",
        "outputId": "fa8393c1-638c-4cb6-cdcc-1f2655d76b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--gamma G] [--seed N] [--render] [--log-interval N]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-66ff66aa-021f-4065-a081-c5ed5bc75c51.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "-Cl3Afob0ev7",
        "outputId": "99459f3a-9071-4518-a8cc-3752f7d917de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0961d02b-53e3-432a-9ca4-9ad9dc6b8fdd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0961d02b-53e3-432a-9ca4-9ad9dc6b8fdd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving naive (1).py to naive (1).py\n",
            "Saving naive.py to naive.py\n",
            "Saving text_classification.py to text_classification.py\n",
            "Saving mnist.py to mnist.py\n",
            "Saving classifiers.py to classifiers.py\n",
            "Saving cart_pole.py to cart_pole.py\n",
            "Saving agent_a_deep_q_learning.pth to agent_a_deep_q_learning.pth\n",
            "Saving tree.py to tree.py\n",
            "Saving horseColicTest.txt to horseColicTest.txt\n",
            "Saving testSet.txt to testSet.txt\n",
            "Saving horseColicTraining.txt to horseColicTraining.txt\n",
            "Saving logistic.py to logistic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            if args.dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
        "                        help='number of epochs to train (default: 14)')\n",
        "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
        "                        help='learning rate (default: 1.0)')\n",
        "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
        "                        help='Learning rate step gamma (default: 0.7)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
        "                        help='quickly check a single pass')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                        help='For Saving the current Model')\n",
        "    args = parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {'batch_size': args.batch_size}\n",
        "    test_kwargs = {'batch_size': args.test_batch_size}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "uPc-EmIeyrL9",
        "outputId": "02fae530-0851-4d22-fa3f-8db3f951e07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--batch-size N] [--test-batch-size N] [--epochs N] [--lr LR]\n",
            "                                [--gamma M] [--no-cuda] [--no-mps] [--dry-run] [--seed S]\n",
            "                                [--log-interval N] [--save-model]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-66ff66aa-021f-4065-a081-c5ed5bc75c51.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/naive_bayes/mnist.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAdPha8C1D3p",
        "outputId": "3268925e-c24e-4d7a-8125-3ff76404e595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "100% 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "100% 28.9k/28.9k [00:00<00:00, 471kB/s]\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "100% 1.65M/1.65M [00:00<00:00, 4.44MB/s]\n",
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "100% 4.54k/4.54k [00:00<00:00, 17.2MB/s]\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.282550\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.384914\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.967074\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.588459\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.345539\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.484442\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.270104\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.669635\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.241586\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.318341\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.264351\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.207598\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.299642\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.159671\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.288941\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.203424\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.262013\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.227533\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.291220\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.099521\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.263692\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.066572\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.154803\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.150762\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.361122\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.332677\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.073468\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.174286\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.117523\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.154366\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.181513\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.135345\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.207951\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.103249\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.359335\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.252157\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.192249\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.173950\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.076620\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.111901\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.090776\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.187854\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.261266\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.165231\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.149449\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.009082\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.102923\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.271631\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.327486\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.203496\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.153337\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.056244\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.175178\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.112652\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.159967\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.085729\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.167831\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.205799\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.195169\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.058884\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.057171\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.035683\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.372734\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.098496\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.071618\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.103416\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.116153\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.027332\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.053808\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.037318\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.068005\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.281179\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.051724\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.023922\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.149639\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.098743\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.298934\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.057322\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.118046\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.115174\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.148985\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.235382\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.181444\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.044161\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.095670\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.035218\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.201169\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.171172\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.226402\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.108016\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.052976\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.080736\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.055334\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.049509\n",
            "\n",
            "Test set: Average loss: 0.0492, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.132127\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.042967\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.020400\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.302222\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.031378\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.071762\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.113276\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.148471\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.045847\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.023250\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.034937\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.025030\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.027051\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.080308\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.030036\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.207444\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.173820\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.061375\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.072254\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.070128\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.091593\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.055204\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.369936\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.015210\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.159920\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.003920\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.143896\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.050268\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.015227\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.064913\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.063639\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.097111\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.075410\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.039103\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.023918\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.080003\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.011417\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.176424\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.018671\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.222611\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.065210\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.155826\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.086226\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.066616\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.030940\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.203068\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.141170\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.118101\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.074844\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.009844\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.026347\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.101999\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.117383\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.202347\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.069765\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.056658\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.004528\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.044450\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.038675\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.086662\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.025796\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.045386\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.026185\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.048236\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.051648\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.032321\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.042041\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.124703\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.026482\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.035515\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.128736\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.018412\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.020935\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.063031\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.084497\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.161261\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.098894\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.051013\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.013271\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.040210\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.012514\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.069178\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.008266\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.237234\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.087422\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.021156\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.013803\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.201312\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.020696\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.119806\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.014526\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.007417\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.072742\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.065753\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.047053\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.023177\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.153372\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.026976\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.012281\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.010378\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.078618\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.010492\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.051953\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.007753\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.011185\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.025546\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.076453\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.042523\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.012208\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.053675\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.010285\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.058993\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.028570\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.064591\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.009478\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.019940\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.069394\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.014679\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.030345\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.376392\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.085434\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.037245\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.156125\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.056667\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.035755\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.089339\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.003455\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.008875\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.105292\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.051471\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.129165\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.021440\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.012211\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.163022\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.005977\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.007750\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.033226\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.058516\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.033557\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.203653\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.005521\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.121820\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.069773\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.060871\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.045229\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.005658\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.004674\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.012901\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.008676\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.028286\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.181355\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.016746\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.191653\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.152132\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.020847\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.048586\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.033699\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.023446\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.066072\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.065807\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.129917\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.055765\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.011951\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.022135\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.005490\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.096028\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.013471\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.001729\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.061427\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.013453\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.058352\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.010995\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.098934\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.013019\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.061323\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.009709\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.121403\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.278465\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.072719\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.029147\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.160802\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.116776\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.055676\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.025042\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.021902\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.118257\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.024060\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.047689\n",
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.008838\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.022242\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.051502\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.021370\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.018200\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.134491\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001759\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.002638\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.152788\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.005511\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.005407\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.099245\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.005147\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.015500\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.066130\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.018896\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.020328\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.032264\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.027707\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.092793\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.036909\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.060184\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.066514\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.007247\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.005884\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.016948\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.001666\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.077894\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.012490\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.057686\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.013705\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.011007\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.053229\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.009262\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.053189\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.025786\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.113987\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.036333\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.038085\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.122304\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.026947\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.048957\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.009110\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.104989\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.106505\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.034470\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.049841\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.043151\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.191254\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.003510\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.026989\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.038067\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.099193\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.065044\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.053895\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.078613\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001352\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.075029\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.002727\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.007857\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005612\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.062698\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.005436\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.030387\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.018851\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.004625\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.004156\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.075253\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.015356\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.008481\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.002011\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.017910\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.056635\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.008436\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.097854\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.010560\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.010060\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.026224\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.001273\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.014007\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.011140\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.006897\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.012619\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.003597\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.008905\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.055503\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.023653\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.034474\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.035999\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.011509\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.079983\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.007735\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.028344\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.086285\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.031660\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.033402\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.007011\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.009776\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.001113\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.021571\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.056026\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.077719\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.026668\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.096068\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002306\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.000865\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.056586\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.041442\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.030846\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.005546\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.012808\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.025024\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.025850\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.043216\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004459\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.076844\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.004100\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.001772\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.045866\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.110464\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.081061\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.013851\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.028702\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.051628\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.003495\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.031034\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.016046\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.002109\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.062210\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.036442\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.024817\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.038184\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.002256\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.005895\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001089\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.011756\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.125959\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.074843\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.045109\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.006960\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.001891\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.065195\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.000932\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.002625\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.149918\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.007265\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.076024\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.002025\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.007385\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.012643\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.006744\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.016009\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.002255\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.048437\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.004466\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.004682\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.002117\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.007408\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.022688\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.014194\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.119674\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.072253\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.076127\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.008272\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.004392\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.042393\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.002248\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.031827\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.013189\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.027747\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.037125\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.010939\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.142951\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.013398\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.041481\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.000800\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.010407\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.026205\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.004098\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.020668\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.000785\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.005150\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.003389\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.060958\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.008134\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.008812\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.001493\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.006195\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.116917\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.057611\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.372539\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.071354\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.014490\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.097922\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.036268\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.006416\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.006307\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.028338\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001382\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.012572\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.005133\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.011714\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.037182\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.062596\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.038885\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.004670\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.003322\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.002440\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.053259\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.064390\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.062136\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.039642\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.050584\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.018464\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.008743\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.004671\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.095938\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.014705\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.067864\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.003150\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.065849\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.000668\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.010880\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.002087\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.039888\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.012284\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.008474\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.045310\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.002004\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.057624\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.022743\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.044833\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.007333\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.040192\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.012021\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.161227\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.060951\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.094118\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.002998\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.137281\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.040434\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.001357\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.050685\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.037317\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.031386\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.041049\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.003350\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.004840\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024445\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.051065\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.031409\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.007254\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.083391\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.207538\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.041180\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.083871\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.013079\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.010306\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.007010\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.027145\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.223546\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.009456\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.025850\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.001075\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.013722\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.069219\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.016003\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.037328\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.030222\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.024450\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.005576\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.047009\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.026695\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.093552\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.037128\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.037411\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.005208\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.012259\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.067611\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.005039\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.030920\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.007329\n",
            "\n",
            "Test set: Average loss: 0.0286, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.012990\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.246346\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.003206\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.031062\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.046119\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.133024\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.054239\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.003279\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.031116\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.023823\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001987\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.000417\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.008617\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.018578\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.011135\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.001027\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.011862\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.005456\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.030432\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.047173\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.028217\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.000872\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.035011\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.006212\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.001636\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.000920\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.001741\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.042304\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.114067\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.004151\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.033479\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.001900\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.000709\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.008267\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.010338\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.004317\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.047783\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.002887\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.002844\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.018029\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.077659\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.016492\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.000982\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.001066\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.002617\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.021962\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.011291\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.107237\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.117148\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.007259\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.010870\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.065825\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.039264\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.002716\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.001706\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.002222\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.006717\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.001401\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.014321\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.064039\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.031231\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.051425\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.031009\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.022042\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.012298\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.033854\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.002202\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.004884\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.056607\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.039056\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.070171\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.050804\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.009239\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.009226\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.038400\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.007865\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.053002\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.070686\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.020860\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.004253\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.028178\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.006081\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.054142\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.010021\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.000421\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.014712\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.054278\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.053715\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.006956\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.077531\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.009202\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.028280\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.110173\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.042364\n",
            "\n",
            "Test set: Average loss: 0.0289, Accuracy: 9916/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.047471\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.012314\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.003853\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.017227\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.029746\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.013100\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.003634\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.001300\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.001474\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.005359\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.040236\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.025332\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.000621\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.064704\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.005345\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.001583\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.058070\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.020121\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.002405\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.002566\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.038497\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.028486\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.031938\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.002547\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.004966\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.000951\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.005084\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.048609\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.026417\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.001102\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.004958\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.000535\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.007775\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.003251\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.010573\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.010232\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.045062\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.003448\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.132329\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.046217\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.009967\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.007577\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.107469\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.222375\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.020402\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.006800\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.066698\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.016807\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.058127\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.021606\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000579\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.020532\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.038516\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.136114\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.001742\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.033953\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.014329\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.005412\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.038958\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.008180\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.001217\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.005932\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.283854\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.023751\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.131750\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.010098\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.376084\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.004480\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.020053\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.022767\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001387\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.060253\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.021291\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.001123\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.017566\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.008862\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.011521\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.018292\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.015270\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.013799\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.002814\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.009082\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.005950\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.006856\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.104756\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.020585\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.008537\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.014892\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.007031\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.002841\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000772\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.002524\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.121548\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.028734\n",
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.003192\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.003203\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.004177\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.014046\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.005807\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.005657\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001695\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.001588\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.002676\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.082758\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.041965\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.003538\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.001540\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.001363\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.088540\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.090750\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.009990\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.017642\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.004180\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.009794\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.038585\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.148544\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.016369\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.002018\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.026867\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.001975\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.033596\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.011792\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.006888\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.128095\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.008014\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.065089\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.003295\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.003406\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.059839\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.000937\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.034766\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.011138\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.052341\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.017591\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.013622\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.016882\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.001651\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.056833\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.001265\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.010867\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.024562\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.006147\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.001286\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.002749\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.012883\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.003942\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.009989\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.003176\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.003356\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.005871\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.120238\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.009182\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.003128\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.021227\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.119963\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.022647\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001069\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.009747\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.002451\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.015621\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.014762\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.050398\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.003486\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.004995\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.005077\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.144681\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.009493\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.003027\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.002681\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.003771\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.106740\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.000921\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.369868\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.122095\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002062\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.013106\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.010872\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.033576\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.051662\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.085904\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.002487\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.017097\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.011315\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.004289\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.055290\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.003482\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.007084\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.002182\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9922/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.028973\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.002065\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.030969\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.009379\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.041962\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.005484\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.106212\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.009051\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.007623\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.015250\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.029970\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.010339\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.036999\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.080497\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.010189\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.005262\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.075463\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.009837\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001807\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.002172\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.028808\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.019000\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.003101\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.024049\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.002174\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.039041\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.109355\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.000421\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.037814\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.007287\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.005097\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.015046\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.033087\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.008947\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.026821\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.016258\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.003233\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.038680\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.009909\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.088057\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001053\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.017616\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.069688\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.001862\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.004384\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.028989\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.048483\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.013370\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.022154\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.006276\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.005805\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.002043\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.007601\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.005727\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.007732\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.006308\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.089557\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.176567\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.001945\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.006717\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.004784\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.011151\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.014387\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.003818\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.000275\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.003572\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.007603\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.019445\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.071317\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.006025\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016459\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.006565\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.028565\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.172547\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.011380\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.002801\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.007594\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001111\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.008122\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.001008\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001108\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.002876\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.002993\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.013184\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.000792\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.010209\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.005389\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.000512\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.013428\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.000768\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.019542\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.014918\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.004671\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.010981\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 9921/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.003005\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.060620\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.003759\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.000148\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.031408\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.057313\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.000460\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.011806\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.031937\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.026256\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.066783\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.002397\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.018674\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.005117\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.038880\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.002150\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.070451\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.003477\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.024300\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.000920\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.031896\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.002831\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.001374\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.041085\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.026729\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.001667\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.070504\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.022425\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.004182\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.063284\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.005992\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.019541\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.024282\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.019166\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.009004\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.024691\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.000848\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.009152\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.004001\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.009729\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.011140\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.051124\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.005478\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.001931\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.009324\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.001842\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.008768\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.032377\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.003993\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.001144\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.007329\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.008834\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.001599\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.005935\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.012409\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.001401\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.001498\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.044772\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.002091\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.089168\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.005494\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.003792\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.002255\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.002414\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.037203\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.002704\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.005886\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.002768\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.077381\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.001253\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.004165\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.011313\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.107438\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.006481\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.051929\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.083117\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.002517\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.047250\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.032484\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.032038\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.003649\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.002845\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.003636\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.006033\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.000672\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.082357\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.009667\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.005834\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.005074\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.016274\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.014346\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.007081\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.007862\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.002472\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 9921/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.043078\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.000896\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.028588\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.070391\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.002895\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.001487\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.000652\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.027560\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.004210\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.078284\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.007476\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.002607\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.002007\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.007151\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.017435\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.020728\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.021303\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.015045\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.048186\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.022278\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.003751\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.006383\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.045664\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.005885\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.002460\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.008549\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.008563\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.016472\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.001172\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.021289\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.000548\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.002383\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.011865\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.020022\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.005303\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.063194\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.013810\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.010373\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.017815\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.021906\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.017053\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.058161\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.147900\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.005687\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.004255\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.001037\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.004785\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.010060\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.020569\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.005941\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.002634\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.001519\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.026920\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.000970\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.012443\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.046350\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.005125\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.002348\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.000312\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.033422\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.001198\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.004389\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.005996\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.002543\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.060537\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.025301\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.026045\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.009365\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.000868\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.003877\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.001285\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.008161\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.005442\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.000825\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.014218\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.008627\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.044465\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.001692\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.000329\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.009927\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.090996\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.006445\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.045340\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.016177\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.002319\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.017018\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.016214\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.011458\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.054456\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.011789\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.061526\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.001104\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.008151\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.049411\n",
            "\n",
            "Test set: Average loss: 0.0265, Accuracy: 9922/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.018589\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.032922\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.029119\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.009886\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.004585\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.002528\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.013440\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.008849\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.040728\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.052653\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.005257\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.146673\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.047030\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.002142\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.008016\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.022964\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.024791\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.052741\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.000577\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.006048\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.025586\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.024824\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.006497\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.001031\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.005667\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.010149\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.009180\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.001291\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.112006\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.002446\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.005570\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.015053\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.016401\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.016390\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.000868\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.019641\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.003369\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.001544\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.013461\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.002439\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.003841\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.004933\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.004651\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.216460\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.002123\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.122211\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.116916\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.034244\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.016987\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.002838\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.011929\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.031338\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.001120\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.002921\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.004538\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.022443\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.029133\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.011795\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.002140\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.027238\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.001872\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.048487\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.011078\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.169516\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.097696\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.008212\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.094359\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.050701\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.009898\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.072473\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.005289\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.022715\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.002634\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.002766\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.180886\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.002006\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.022616\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.003992\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.009447\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.006570\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.004495\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.048048\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.070711\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.107796\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.008672\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.007505\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.043064\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.001091\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.027928\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.007252\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.015263\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.002427\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.036962\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.005274\n",
            "\n",
            "Test set: Average loss: 0.0266, Accuracy: 9924/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000492\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.069772\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.001666\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.020686\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.004120\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.068172\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.012596\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.008599\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.020514\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.140717\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.010625\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.033773\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.001939\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.008948\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.009246\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.012688\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.005290\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.000923\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.000807\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.017562\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.056957\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.017322\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.009742\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.011560\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.003714\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.005305\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.003958\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.000849\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.001176\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.002889\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.001632\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.072716\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.002114\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.010434\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.006775\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.008490\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.150218\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.015746\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.018150\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.017988\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.056345\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.007463\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.000827\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.046365\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.008956\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.011820\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.002940\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.021057\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.014410\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.003844\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.008006\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.052809\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.006696\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.008031\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.012562\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.014415\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.010091\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.041706\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.015495\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.006823\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.023677\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.003037\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.020951\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.003975\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.010838\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.008495\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.003251\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.001291\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.041514\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.049548\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.000968\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.000996\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.012860\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.118288\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.009814\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.065756\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.002584\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.010457\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.003548\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.013386\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.017648\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.005002\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.033899\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.003648\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.001884\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.038645\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.050925\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.006990\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.029772\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.074346\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.002862\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.004988\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.002619\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.001862\n",
            "\n",
            "Test set: Average loss: 0.0265, Accuracy: 9929/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BvasHCQJx7Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "COxAdlKwx7nT",
        "outputId": "64ab2782-0ad2-4965-f77a-c98c32cc3243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e6525e28-108d-42c0-a128-d8a35dc0c7ff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e6525e28-108d-42c0-a128-d8a35dc0c7ff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving play.py to play.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'play.py': b'\"\"\"Play against an agent.\"\"\"\\nfrom src.utils import print_args\\nfrom src.utils import load_checkpoint\\nfrom src.argparser import argument_parser\\nfrom src.model import Model\\nfrom src.environment import TicTacToe\\n\\n\\nif __name__ == \"__main__\":\\n\\n    args = argument_parser()\\n\\n    print_args(args=args)\\n\\n    model = Model(args=args)\\n    load_checkpoint(model=model, args=args)\\n\\n    env = TicTacToe(size=args.field_size)\\n    env.play(model=model)\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/machine_learning-master/TTT/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9Ed8ZUv4NZP",
        "outputId": "c937eeb2-c40d-43ae-fd79-3f9001536cf4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-20 14:04:43.948566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-20 14:04:43.967759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-20 14:04:43.973921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-20 14:04:43.988197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-20 14:04:45.055420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Configuration:\n",
            "\n",
            "random_seed.....................42\n",
            "algorithm.......................policy_gradient\n",
            "learning_rate...................0.0001\n",
            "num_episodes....................10000\n",
            "gamma...........................1.0\n",
            "epsilon.........................1.0\n",
            "epsilon_min.....................0.01\n",
            "decay_rate......................0.9999\n",
            "memory_size.....................500000\n",
            "batch_size......................128\n",
            "field_size......................3\n",
            "dropout_probability.............0.0\n",
            "num_layers......................1\n",
            "num_hidden_units................128\n",
            "model_name......................None\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/machine_learning-master/TTT/train.py\", line 30, in <module>\n",
            "    train(env=env, agent_a=agent_a, agent_b=agent_b, args=args)\n",
            "  File \"/content/machine_learning-master/TTT/src/trainer.py\", line 40, in train\n",
            "    save_checkpoint(model=agent_a.model, model_name=\"agent_a\", args=args)\n",
            "  File \"/content/machine_learning-master/TTT/src/utils.py\", line 32, in save_checkpoint\n",
            "    torch.save(obj=model.state_dict(), f=model_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 849, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 716, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 687, in __init__\n",
            "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
            "RuntimeError: Parent directory src/weights does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/machine_learning-master/TTT/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WID-_h3M9AVy",
        "outputId": "68968810-fee3-4b33-8008-12cf1227a530"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-20 13:56:12.620934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-20 13:56:12.641100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-20 13:56:12.646854: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-20 13:56:12.662026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-20 13:56:13.755230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Configuration:\n",
            "\n",
            "random_seed.....................42\n",
            "algorithm.......................policy_gradient\n",
            "learning_rate...................0.0001\n",
            "num_episodes....................10000\n",
            "gamma...........................1.0\n",
            "epsilon.........................1.0\n",
            "epsilon_min.....................0.01\n",
            "decay_rate......................0.9999\n",
            "memory_size.....................500000\n",
            "batch_size......................128\n",
            "field_size......................3\n",
            "dropout_probability.............0.0\n",
            "num_layers......................1\n",
            "num_hidden_units................128\n",
            "model_name......................None\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/machine_learning-master/TTT/train.py\", line 30, in <module>\n",
            "    train(env=env, agent_a=agent_a, agent_b=agent_b, args=args)\n",
            "  File \"/content/machine_learning-master/TTT/src/trainer.py\", line 40, in train\n",
            "    save_checkpoint(model=agent_a.model, model_name=\"agent_a\", args=args)\n",
            "  File \"/content/machine_learning-master/TTT/src/utils.py\", line 32, in save_checkpoint\n",
            "    torch.save(obj=model.state_dict(), f=model_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 849, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 716, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 687, in __init__\n",
            "    super().__init__(torch._C.PyTorchFileWriter(self.name))\n",
            "RuntimeError: Parent directory src/weights does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/machine_learning-master/TTT/play.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mIKnjLu2xuT",
        "outputId": "36921ba6-6d54-4b9e-a7e8-84d0554e5e6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "\n",
            "random_seed.....................42\n",
            "algorithm.......................policy_gradient\n",
            "learning_rate...................0.0001\n",
            "num_episodes....................10000\n",
            "gamma...........................1.0\n",
            "epsilon.........................1.0\n",
            "epsilon_min.....................0.01\n",
            "decay_rate......................0.9999\n",
            "memory_size.....................500000\n",
            "batch_size......................128\n",
            "field_size......................3\n",
            "dropout_probability.............0.0\n",
            "num_layers......................1\n",
            "num_hidden_units................128\n",
            "model_name......................None\n",
            "\n",
            "/content/machine_learning-master/TTT/src/utils.py:52: UserWarning: \n",
            "Model checkpoint 'model' not found. Continuing with random weights.\n",
            "\n",
            "  warnings.warn(f\"\\nModel checkpoint '{checkpoint_name}' not found. \" \"Continuing with random weights.\\n\")\n",
            "\n",
            "Game started.\n",
            "\n",
            "Machine\n",
            "\n",
            "  .  .  .\n",
            "  .  .  .\n",
            "  x  .  .\n",
            "\n",
            "Enter an index between [0, 8]: 5\n",
            "\n",
            "  .  .  .\n",
            "  .  .  o\n",
            "  x  .  .\n",
            "\n",
            "Machine\n",
            "\n",
            "  .  .  .\n",
            "  .  x  o\n",
            "  x  .  .\n",
            "\n",
            "Enter an index between [0, 8]: 5\n",
            "\n",
            "  .  .  .\n",
            "  .  x  o\n",
            "  x  .  .\n",
            "\n",
            "Illegal move. Computer won.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aakEkRY98Is9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/machine_learning-master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRYBdjB68jYi",
        "outputId": "75dd2be5-a9e1-4470-93f4-e4dfb08830f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/machine_learning-master.zip\n",
            "2bc68135efc22cc9cd4c435a49a300a5b6efcbba\n",
            "   creating: machine_learning-master/\n",
            "   creating: machine_learning-master/TTT/\n",
            "  inflating: machine_learning-master/TTT/agent_a_deep_q_learning.pth  \n",
            "  inflating: machine_learning-master/TTT/play.py  \n",
            " extracting: machine_learning-master/TTT/requirements.txt  \n",
            "   creating: machine_learning-master/TTT/runs/\n",
            "   creating: machine_learning-master/TTT/runs/Nov17_22-51-48_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov17_22-51-48_DESKTOP-UHEVTJ6/events.out.tfevents.1731880308.DESKTOP-UHEVTJ6.3560.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_09-40-54_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_09-40-54_DESKTOP-UHEVTJ6/events.out.tfevents.1731919254.DESKTOP-UHEVTJ6.17304.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_09-45-08_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_09-45-08_DESKTOP-UHEVTJ6/events.out.tfevents.1731919508.DESKTOP-UHEVTJ6.23312.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-22-04_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-22-04_DESKTOP-UHEVTJ6/events.out.tfevents.1731921724.DESKTOP-UHEVTJ6.1968.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-23-34_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-23-34_DESKTOP-UHEVTJ6/events.out.tfevents.1731921814.DESKTOP-UHEVTJ6.25080.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-25-14_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-25-14_DESKTOP-UHEVTJ6/events.out.tfevents.1731921914.DESKTOP-UHEVTJ6.7384.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-25-44_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-25-44_DESKTOP-UHEVTJ6/events.out.tfevents.1731921944.DESKTOP-UHEVTJ6.25232.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-28-16_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-28-16_DESKTOP-UHEVTJ6/events.out.tfevents.1731922096.DESKTOP-UHEVTJ6.23956.0  \n",
            "   creating: machine_learning-master/TTT/runs/Nov18_10-30-13_DESKTOP-UHEVTJ6/\n",
            "  inflating: machine_learning-master/TTT/runs/Nov18_10-30-13_DESKTOP-UHEVTJ6/events.out.tfevents.1731922213.DESKTOP-UHEVTJ6.5448.0  \n",
            "   creating: machine_learning-master/TTT/src/\n",
            "   creating: machine_learning-master/TTT/src/__pycache__/\n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/agent.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/argparser.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/deep_q_learning.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/environment.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/model.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/policy_gradient.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/trainer.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/__pycache__/utils.cpython-311.pyc  \n",
            "  inflating: machine_learning-master/TTT/src/agent.py  \n",
            "  inflating: machine_learning-master/TTT/src/argparser.py  \n",
            "  inflating: machine_learning-master/TTT/src/deep_q_learning.py  \n",
            "  inflating: machine_learning-master/TTT/src/environment.py  \n",
            "  inflating: machine_learning-master/TTT/src/model.py  \n",
            "  inflating: machine_learning-master/TTT/src/policy_gradient.py  \n",
            "  inflating: machine_learning-master/TTT/src/trainer.py  \n",
            "  inflating: machine_learning-master/TTT/src/utils.py  \n",
            "   creating: machine_learning-master/TTT/src/weights/\n",
            "  inflating: machine_learning-master/TTT/src/weights/agent_a_deep_q_learning.pth  \n",
            "  inflating: machine_learning-master/TTT/src/weights/agent_b_deep_q_learning.pth  \n",
            "   creating: machine_learning-master/TTT/tests/\n",
            " extracting: machine_learning-master/TTT/tests/__init__.py  \n",
            "  inflating: machine_learning-master/TTT/tests/test_tictactoe.py  \n",
            "  inflating: machine_learning-master/TTT/train.py  \n",
            "   creating: machine_learning-master/TicTacToe/\n",
            "  inflating: machine_learning-master/agent_a_deep_q_learning.pth  \n",
            "  inflating: machine_learning-master/cart_pole.py  \n",
            "  inflating: machine_learning-master/classifiers.py  \n",
            "   creating: machine_learning-master/dtree/\n",
            "  inflating: machine_learning-master/dtree/tree.py  \n",
            "   creating: machine_learning-master/logistic_regression/\n",
            "  inflating: machine_learning-master/logistic_regression/horseColicTest.txt  \n",
            "  inflating: machine_learning-master/logistic_regression/horseColicTraining.txt  \n",
            "  inflating: machine_learning-master/logistic_regression/logistic.py  \n",
            "  inflating: machine_learning-master/logistic_regression/testSet.txt  \n",
            "  inflating: machine_learning-master/mnist.py  \n",
            "   creating: machine_learning-master/naive_bayes/\n",
            "   creating: machine_learning-master/naive_bayes/email/\n",
            "   creating: machine_learning-master/naive_bayes/email/ham/\n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/1.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/10.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/11.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/12.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/13.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/14.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/15.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/16.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/17.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/18.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/19.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/2.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/20.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/21.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/22.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/23.txt  \n",
            " extracting: machine_learning-master/naive_bayes/email/ham/24.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/25.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/3.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/4.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/5.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/6.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/7.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/8.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/ham/9.txt  \n",
            "   creating: machine_learning-master/naive_bayes/email/spam/\n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/1.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/10.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/11.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/12.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/13.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/14.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/15.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/16.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/17.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/18.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/19.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/2.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/20.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/21.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/22.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/23.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/24.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/25.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/3.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/4.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/5.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/6.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/7.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/8.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/email/spam/9.txt  \n",
            "  inflating: machine_learning-master/naive_bayes/naive.py  \n",
            "  inflating: machine_learning-master/text_classification.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XpyUpJio8kPS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9hTiG9_8j9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "w7Z5z7g95mGC",
        "outputId": "df1fb1ed-f695-4afe-d36f-e94c318eda3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret secretName does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-245876ae012f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'secretName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret secretName does not exist."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMhjGiDHDiSqqLmb5xuvskf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}